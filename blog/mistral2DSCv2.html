<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>mistral2DSCv2</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {  background-color: #f8f8f8; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ef2929; } /* Alert */
    code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #204a87; } /* Attribute */
    code span.bn { color: #0000cf; } /* BaseN */
    code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4e9a06; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #8f5902; font-style: italic; } /* Comment */
    code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
    code span.dt { color: #204a87; } /* DataType */
    code span.dv { color: #0000cf; } /* DecVal */
    code span.er { color: #a40000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #0000cf; } /* Float */
    code span.fu { color: #204a87; font-weight: bold; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
    code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
    code span.ot { color: #8f5902; } /* Other */
    code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
    code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
    code span.ss { color: #4e9a06; } /* SpecialString */
    code span.st { color: #4e9a06; } /* String */
    code span.va { color: #000000; } /* Variable */
    code span.vs { color: #4e9a06; } /* VerbatimString */
    code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="/dev/fd/63" />
  
  <head>
  	<link rel="stylesheet" type="text/css" href="../style.css" />
        <link
            rel="icon"
            type="image/png"
            href="https://static-00.iconduck.com/assets.00/cat-face-with-wry-smile-emoji-2048x2048-mh4ul5mr.png"
        />
        <meta charset="UTF-8" />
        <meta
            name="description"
            content=""
        />
        <meta name="keywords" content="ksagar, vovw, atharva" />
        <meta name="author" content="ksagar" />
        <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>

</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#from-mistral-to-deepseekcoderv2-lite"
id="toc-from-mistral-to-deepseekcoderv2-lite">from mistral to
DeepSeekCoderv2 lite</a>
<ul>
<li><a href="#rope" id="toc-rope">RoPE</a></li>
<li><a href="#feed-forward-network" id="toc-feed-forward-network">Feed
Forward Network</a></li>
<li><a href="#the-transformer-block" id="toc-the-transformer-block">The
Transformer Block</a></li>
<li><a href="#the-mistral-class" id="toc-the-mistral-class">The Mistral
Class</a></li>
</ul></li>
</ul>
</nav>
<h1 id="from-mistral-to-deepseekcoderv2-lite">from mistral to
DeepSeekCoderv2 lite</h1>
<p>a wip documenting my work to go take my inference engine from mistral
to DeepSeekCoderv2 which is a SOTA coding model ## Attention Block</p>
<p>“Attention is all you need” memes.</p>
<p>This class has two functions: one initializes the query, key, value
(q, k, v) and output projections as linear layers, and also RoPE for
positional encoding. The second function implements the attention
mechanisms, namely it does the following:</p>
<ul>
<li>It projects the q, k, v values. I like to think of the query as what
we are looking for, key as what we have, and value as the actual
information.</li>
<li>Applies RoPE.</li>
<li>Uses the <a
href="https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.fast.scaled_dot_product_attention.html#mlx.core.fast.scaled_dot_product_attention">scaled_dot_product_attention</a>
for fast attention computation.</li>
<li>Projects the output back to the model dimension (ensures that the
output of one layer can be used as input for the next layer).</li>
</ul>
<h3 id="rope"><a
href="https://paperswithcode.com/method/rope">RoPE</a></h3>
<ul>
<li>Type of positional embedding which encodes absolute positional
information with the rotation of a matrix.</li>
<li>Basically helps the model understand the order of words in a
sequence.</li>
</ul>
<h2 id="feed-forward-network">Feed Forward Network</h2>
<p>i dont have more to expand lol prolly look it up</p>
<h2 id="the-transformer-block">The Transformer Block</h2>
<ul>
<li>We create instances of attention and feedforward blocks. We also
create two <a
href="https://ml-explore.github.io/mlx/build/html/python/nn/_autosummary/mlx.nn.RMSNorm.html#mlx-nn-rmsnorm">RMSNorm</a>
layers for normalization.</li>
</ul>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>r, cache <span class="op">=</span> <span class="va">self</span>.attention(<span class="va">self</span>.attention_norm(x), mask, cache)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> x <span class="op">+</span> r</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="va">self</span>.feed_forward(<span class="va">self</span>.ffn_norm(h))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> h <span class="op">+</span> r</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> out, cache</span></code></pre></div>
<ul>
<li><p>The input goes through attention_norm, then attention to make
result (r).</p></li>
<li><p>Then we add the result back to the input, which is a residual
connection (h).</p></li>
<li><p>Then this goes through feedforward norm and feedforward network
to make result (r).</p></li>
<li><p>We add this result back to the h, which is the output
(out).</p></li>
<li><p>We normalize to stabilize the network and speed up training.
(Play around with different normalization to build up the
intuition.)</p></li>
<li><p>The cache is used for effective autoregressive generation.
Basically, it stores key and value tensors to avoid recomputing them for
each new token.</p></li>
</ul>
<h2 id="the-mistral-class">The Mistral Class</h2>
<ul>
<li>Puts everything together, creates the token embedding, a stack of
the transformer blocks for the number of layers, final normalization
layer, and an output projection.</li>
<li>Converts input to embeddings.</li>
<li>Creates a mask if processing multiple tokens.</li>
<li>Passes the embeddings through each transformer block.</li>
<li>Applies final normalization and projects to vocabulary size.</li>
</ul>
</body>
</html>
