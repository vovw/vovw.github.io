<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>rl</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="/dev/fd/63" />
  <head>
      <link rel="stylesheet" type="text/css" href="../style.css" />
      <link rel="icon" type="image/png" href="cat.png" />

      <meta charset="UTF-8" />
      <meta name="description" content="personal website for ksagar" />
      <meta
          name="keywords"
          content="ksagar, vovw, atharva, k7agar, atharva kshirsagar"
      />
      <meta name="author" content="ksagar" />

      <!-- Open Graph / Facebook -->
      <meta property="og:type" content="website" />
      <meta property="og:url" content="https://www.ksagar.me/" />
      <meta property="og:title" content="your friendly neighbourhood engineer" />
      <meta property="og:description" content="personal website for ksagar" />
      <meta
          property="og:image"
          content=""
      />

      <!-- Twitter -->
      <meta name="twitter:card" content="summary_large_image" />
      <meta name="twitter:url" content="https://www.ksagar.me/" />
      <meta name="twitter:description" content="personal website for ksagar" />

      <meta name="theme-color" content="#c4c0aa" />
      <link rel="canonical" href="https://www.ksagar.me/" />
  </head>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction-to-rl"
id="toc-introduction-to-rl">introduction to rl</a></li>
<li><a href="#terms" id="toc-terms">terms</a>
<ul>
<li><a href="#model-free-rl" id="toc-model-free-rl">model-free
rl</a></li>
<li><a href="#model-based-rl" id="toc-model-based-rl">model-based
rl</a></li>
<li><a href="#markov-decision-process-mdp"
id="toc-markov-decision-process-mdp">Markov Decision Process
(MDP)</a></li>
</ul></li>
<li><a href="#algorithms" id="toc-algorithms">algorithms</a>
<ul>
<li><a href="#vanilla-policy-gradient"
id="toc-vanilla-policy-gradient">Vanilla Policy Gradient</a></li>
</ul></li>
<li><a href="#applications" id="toc-applications">applications</a></li>
<li><a href="#examples" id="toc-examples">examples</a></li>
<li><a href="#rl-based-control" id="toc-rl-based-control">rl based
control</a></li>
</ul>
</nav>
<h1 id="introduction-to-rl">introduction to rl</h1>
<p>The agent is acting in an environment. How the environment reacts to
certain actions is defined by a model which we may or may not know. The
agent can stay in one of many states of the environment, and choose to
take one of many actions () to switch from one state to another. Which
state the agent will arrive in is decided by transition probabilities
between states. Once an action is taken, the environment delivers a
reward as feedback</p>
<h1 id="terms">terms</h1>
<h3 id="model-free-rl">model-free rl</h3>
<ul>
<li>do not reply on model of the enviroment</li>
<li>directly learn the policy or value function from expirence</li>
<li>generally easier to implement and tune</li>
<li>more samples required to learn effectively</li>
</ul>
<h3 id="model-based-rl">model-based rl</h3>
<ul>
<li>uses a model of the enviroment</li>
<li>predicts the state transition and rewards</li>
<li>allows the agent to plan ahead, potentially improving sample
efficiency</li>
<li>may introduce biases that degrade performance in the real
environment</li>
</ul>
<h3 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h3>
<ul>
<li>math framework used ot model decision-making</li>
<li>has - STATES(S), ACTIONS(A), TRANSITION PROBS(P), REWARDS(R),
DISCOUNT FACTOR (γ)</li>
</ul>
<h1 id="algorithms">algorithms</h1>
<h2 id="vanilla-policy-gradient">Vanilla Policy Gradient</h2>
<ul>
<li>stochastic policy</li>
</ul>
<p>π(a|s,θ) Prob(action|state, params)</p>
<ul>
<li>change params so as to maximise rewards</li>
<li>different from TD learning, no neeed for calculating Q value or V
value.</li>
<li>better for continuous states</li>
</ul>
<h1 id="applications">applications</h1>
<h1 id="examples">examples</h1>
<h1 id="rl-based-control">rl based control</h1>
</body>
</html>
