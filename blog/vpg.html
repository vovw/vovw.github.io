<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>vpg</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {  background-color: #f8f8f8; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ef2929; } /* Alert */
    code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #204a87; } /* Attribute */
    code span.bn { color: #0000cf; } /* BaseN */
    code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4e9a06; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #8f5902; font-style: italic; } /* Comment */
    code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
    code span.dt { color: #204a87; } /* DataType */
    code span.dv { color: #0000cf; } /* DecVal */
    code span.er { color: #a40000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #0000cf; } /* Float */
    code span.fu { color: #204a87; font-weight: bold; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
    code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
    code span.ot { color: #8f5902; } /* Other */
    code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
    code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
    code span.ss { color: #4e9a06; } /* SpecialString */
    code span.st { color: #4e9a06; } /* String */
    code span.va { color: #000000; } /* Variable */
    code span.vs { color: #4e9a06; } /* VerbatimString */
    code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="/dev/fd/63" />
  <head>
      <link rel="stylesheet" type="text/css" href="../style.css" />
      <link rel="icon" type="image/png" href="cat.png" />

      <meta charset="UTF-8" />
      <meta name="description" content="personal website for ksagar" />
      <meta
          name="keywords"
          content="ksagar, vovw, atharva, k7agar, atharva kshirsagar"
      />
      <meta name="author" content="ksagar" />

      <!-- Open Graph / Facebook -->
      <meta property="og:type" content="website" />
      <meta property="og:url" content="https://www.ksagar.me/" />
      <meta property="og:title" content="your friendly neighbourhood engineer" />
      <meta property="og:description" content="personal website for ksagar" />
      <meta
          property="og:image"
          content=""
      />

      <!-- Twitter -->
      <meta name="twitter:card" content="summary_large_image" />
      <meta name="twitter:url" content="https://www.ksagar.me/" />
      <meta name="twitter:description" content="personal website for ksagar" />

      <meta name="theme-color" content="#c4c0aa" />
      <link rel="canonical" href="https://www.ksagar.me/" />
  </head>
  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#vanilla-policy-gradient-vpg-implementation"
id="toc-vanilla-policy-gradient-vpg-implementation">Vanilla Policy
Gradient (VPG) Implementation</a>
<ul>
<li><a href="#tldr" id="toc-tldr">TLDR</a></li>
<li><a href="#short-overview-of-vpg"
id="toc-short-overview-of-vpg">Short Overview of VPG</a></li>
<li><a href="#vpg" id="toc-vpg">VPG</a>
<ul>
<li><a href="#the-formal-statement" id="toc-the-formal-statement">The
formal statement</a></li>
<li><a href="#interpretation"
id="toc-interpretation">Interpretation</a></li>
</ul></li>
<li><a href="#environment" id="toc-environment">Environment</a></li>
<li><a href="#reinforcement-learning-approach"
id="toc-reinforcement-learning-approach">Reinforcement Learning
Approach</a></li>
<li><a href="#cost-function-in-policy-gradient-methods"
id="toc-cost-function-in-policy-gradient-methods">Cost Function in
Policy Gradient Methods</a></li>
<li><a href="#detailed-algorithm-steps"
id="toc-detailed-algorithm-steps">Detailed Algorithm Steps</a></li>
<li><a href="#implementation"
id="toc-implementation">Implementation</a></li>
<li><a href="#implementation-details"
id="toc-implementation-details">Implementation Details</a></li>
<li><a href="#mathematical-explanation-of-the-update-step"
id="toc-mathematical-explanation-of-the-update-step">Mathematical
Explanation of the Update Step</a></li>
</ul></li>
</ul>
</nav>
<h1 id="vanilla-policy-gradient-vpg-implementation">Vanilla Policy
Gradient (VPG) Implementation</h1>
<h2 id="tldr">TLDR</h2>
<p>A simple PyTorch Reinforcement Learning model where an agent learns
to move towards food while avoiding enemies on a 2D grid, based on
log-probabilities and policy updates using backpropagation through
time.</p>
<h2 id="short-overview-of-vpg">Short Overview of VPG</h2>
<ul>
<li><p>Stochastic policy: <span
class="math inline">\(\pi(a|s,\theta)\)</span> where <span
class="math inline">\(\pi\)</span> represents the probability of taking
action <span class="math inline">\(a\)</span> given state <span
class="math inline">\(s\)</span> and parameters <span
class="math inline">\(\theta\)</span>.</p></li>
<li><p>The objective is to change parameters <span
class="math inline">\(\theta\)</span> to maximize expected
rewards.</p></li>
<li><p>Unlike TD learning, thereâ€™s no need for calculating Q-value or
V-value.</p></li>
<li><p>Better suited for continuous state spaces.</p></li>
</ul>
<h2 id="vpg">VPG</h2>
<p>The goal of PG methods is to maximize the expected return:</p>
<p><span class="math inline">\(J(\theta) = \mathbb{E}{\tau \sim
\pi\theta}[\sum_{t=0}^T R(s_t, a_t)]\)</span></p>
<p>where <span class="math inline">\(\tau\)</span> represents a
trajectory, and <span class="math inline">\(R(s_t, a_t)\)</span> is the
reward at time <span class="math inline">\(t\)</span>.</p>
<p>The policy gradient theorem gives us: <span
class="math inline">\(\nabla_\theta J(\theta) = \mathbb{E}{\tau \sim
\pi\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t)
R_t]\)</span></p>
<p>where <span class="math inline">\(R_t = \sum_{t&#39;=t}^T
R(s_{t&#39;}, a_{t&#39;})\)</span> is the return from time <span
class="math inline">\(t\)</span>.</p>
<p>It is fundamental algorithm in RL that provides a way to compute the
gradient of the expected return with respect to the policy parameters.
This is really nice because it allows us to optmize the policy directly,
without needing to estimate action-values.</p>
<h3 id="the-formal-statement">The formal statement</h3>
<p>The intuition behind this is that we need to increase the probability
of actions that lead to good outcomes and decrease the probability of
actions that lead to bad outcomes.</p>
<p>Let <span class="math inline">\(\pi_\theta(a|s)\)</span> be a
stochastic policy parametrized by <span
class="math inline">\(\theta\)</span>, and let <span
class="math inline">\(J(\theta)\)</span> be the expected return when
following this policy. The policy gradient theorem states that:</p>
<p><span class="math inline">\(\nabla_\theta J(\theta) =
\mathbb{E}_{\tau \sim \pi_\theta}[\sum_{t=0}^T \nabla_\theta \log
\pi_\theta(a_t|s_t) R_t]\)</span></p>
<p>where: - <span class="math inline">\(\tau\)</span> is a trajectory
<span class="math inline">\((s_0, a_0, r_0, s_1, a_1, r_1, ...)\)</span>
- <span class="math inline">\(R_t = \sum_{t&#39;=t}^T
r_{t&#39;}\)</span> is the return from time <span
class="math inline">\(t\)</span></p>
<ol type="1">
<li><strong>Log-probability gradient</strong>: <span
class="math inline">\(\nabla_\theta \log \pi_\theta(a_t|s_t)\)</span>
<ul>
<li>This term indicates how the log probability of taking action <span
class="math inline">\(a_t\)</span> in state <span
class="math inline">\(s_t\)</span> changes with respect to the policy
parameters.</li>
</ul></li>
<li><strong>Return</strong>: <span class="math inline">\(R_t\)</span>
<ul>
<li>This is the cumulative reward from time <span
class="math inline">\(t\)</span> onwards.</li>
</ul></li>
<li><strong>Expectation</strong>: <span
class="math inline">\(\mathbb{E}_{\tau \sim \pi_\theta}\)</span>
<ul>
<li>This indicates that weâ€™re averaging over all possible trajectories
under the current policy.</li>
</ul></li>
</ol>
<h3 id="interpretation">Interpretation</h3>
<p>The theorem tells us that to increase the expected return, we should
move the policy parameters in a direction that: 1. Increases the
probability of actions that led to high returns. 2. Decreases the
probability of actions that led to low returns.</p>
<p>The magnitude of the update is proportional to the return, so actions
that led to much better (or worse) outcomes have a larger impact on the
policy update.</p>
<p>The policy gradient theorem works because: 1. It directly optimizes
the quantity we care about (expected return). 2. It doesnâ€™t require
estimating action-values, which can be difficult in large state spaces.
3. It naturally handles continuous action spaces.</p>
<p>While awesome, the policy gradient theorem has some limitations: 1.
High variance: The returns can vary a lot, leading to noisy gradients.
2. Sample inefficiency: It often requires many samples to estimate the
gradient accurately.</p>
<p>These limitations have led to various improvements and
extensions.</p>
<h2 id="environment">Environment</h2>
<p>The environment in this problem is a simple 2D grid-based world where
there are 3 blobs (player, food, and enemy). The objective of the RL
agent is to move towards the food blob while avoiding the enemy blob by
choosing appropriate actions based on its current state.</p>
<p>The environment is represented as a numpy array of size SIZE x SIZE
where each cell can contain one of 3 blobs. The input for the policy
network consists of the relative positions of these 3 blobs in the
current state.</p>
<h2 id="reinforcement-learning-approach">Reinforcement Learning
Approach</h2>
<p>In this code, we use PyTorch to build a simple policy gradient RL
model with a neural network that represents our policy function.</p>
<p>This network takes our state representations as input and outputs a
probability distribution over possible actions. Mathematically, for a
state <span class="math inline">\(s\)</span>:</p>
<p><span class="math inline">\(\pi_\theta(a|s) =
\text{softmax}(f_\theta(s))\)</span></p>
<p>where <span class="math inline">\(f_\theta\)</span> is our neural
network.</p>
<p>The agent then selects an action based on these probabilities using
the Categorical distributionâ€™s sample method.</p>
<h2 id="cost-function-in-policy-gradient-methods">Cost Function in
Policy Gradient Methods</h2>
<p>In policy gradient methods, our objective is to maximize the expected
return. However, in practice/implementation, we often minimize a
loss/cost function that, when minimized, maximizes our expected return.
The cost function for policy gradient methods is defined as:</p>
<p><span class="math inline">\(J(\theta) = -\mathbb{E}_{\tau \sim
\pi_\theta}[\sum_{t=0}^T R(s_t, a_t)]\)</span></p>
<p>Here - <span class="math inline">\(\theta\)</span> represents the
parameters of our policy, <span class="math inline">\(\tau\)</span> is a
trajectory and <span class="math inline">\(R(s_t, a_t)\)</span> is the
reward at time <span class="math inline">\(t\)</span>.</p>
<p>The negative sign is crucial because we typically <strong>perform
gradient descent to minimize this function</strong>, which effectively
maximizes our expected return.</p>
<h2 id="detailed-algorithm-steps">Detailed Algorithm Steps</h2>
<ol type="1">
<li><p><strong>Initialize the Policy Network</strong>: We start by
initializing our policy network <span
class="math inline">\(\pi_\theta(a|s)\)</span> with random weights <span
class="math inline">\(\theta\)</span>.</p></li>
<li><p><strong>Collect Trajectories</strong>: For each episode:</p>
<ol type="a">
<li>Initialize the environment and get the initial state <span
class="math inline">\(s_0\)</span>.</li>
<li>For each time step <span class="math inline">\(t\)</span> in the
episode:
<ul>
<li>Sample action <span class="math inline">\(a_t \sim
\pi_\theta(a|s_t)\)</span></li>
<li>Execute <span class="math inline">\(a_t\)</span> and observe next
state <span class="math inline">\(s_{t+1}\)</span> and reward <span
class="math inline">\(r_t\)</span></li>
<li>Store <span class="math inline">\((s_t, a_t, r_t)\)</span> in the
current trajectory</li>
</ul></li>
<li>End the episode when a terminal state is reached</li>
</ol></li>
<li><p><strong>Compute Returns</strong>: For each trajectory:</p>
<ol type="a">
<li>Initialize <span class="math inline">\(G_T = 0\)</span> (return at
the final step)</li>
<li>For <span class="math inline">\(t = T-1\)</span> to 0:
<ul>
<li>Compute <span class="math inline">\(G_t = r_t + \gamma
G_{t+1}\)</span> This gives us the discounted return for each time
step.</li>
</ul></li>
</ol></li>
<li><p><strong>Normalize Returns</strong> (Optional): To reduce
variance, we often normalize the returns: <span
class="math inline">\(G_t&#39; = \frac{G_t - \mu(G)}{\sigma(G)}\)</span>
where <span class="math inline">\(\mu(G)\)</span> and <span
class="math inline">\(\sigma(G)\)</span> are the mean and standard
deviation of all returns.</p></li>
<li><p><strong>Compute Policy Gradient</strong>: The policy gradient is
computed as: <span class="math inline">\(\nabla_\theta J(\theta) =
-\mathbb{E}_{\tau \sim \pi_\theta}[\sum_{t=0}^T \nabla_\theta \log
\pi_\theta(a_t|s_t) G_t&#39;]\)</span></p>
<p>In practice, we approximate this expectation with our sampled
trajectories: <span class="math inline">\(\nabla_\theta J(\theta)
\approx -\frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \log
\pi_\theta(a_t^i|s_t^i) G_t&#39;^i\)</span></p>
<p>where <span class="math inline">\(N\)</span> is the number of
trajectories.</p></li>
<li><p><strong>Update Policy Parameters</strong>: We update our policy
parameters using gradient descent: <span class="math inline">\(\theta
\leftarrow \theta - \alpha \nabla_\theta J(\theta)\)</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the learning
rate.</p></li>
<li><p><strong>Repeat</strong>: Go back to step 2 and repeat for many
episodes until convergence or a maximum number of episodes is
reached.</p></li>
</ol>
<h2 id="implementation">Implementation</h2>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PolicyNetwork(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(PolicyNetwork, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(input_size, <span class="dv">64</span>),</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">64</span>, <span class="dv">64</span>),</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">64</span>, output_size)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.softmax(<span class="va">self</span>.fc(x), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> select_action(state):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> torch.FloatTensor(state).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> policy_net(state)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> Categorical(probs)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> m.sample()</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> action.item(), m.log_prob(action)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_policy(log_probs, rewards):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    returns <span class="op">=</span> []</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">reversed</span>(rewards):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> r <span class="op">+</span> <span class="fl">0.99</span> <span class="op">*</span> G  <span class="co"># 0.99 is the discount factor</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        returns.insert(<span class="dv">0</span>, G)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    returns <span class="op">=</span> torch.tensor(returns, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(returns) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        returns <span class="op">=</span> (returns <span class="op">-</span> returns.mean()) <span class="op">/</span> (returns.std() <span class="op">+</span> <span class="fl">1e-8</span>)  <span class="co"># Normalization</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    policy_loss <span class="op">=</span> []</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> log_prob, R <span class="kw">in</span> <span class="bu">zip</span>(log_probs, returns):</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        policy_loss.append(<span class="op">-</span>log_prob <span class="op">*</span> R)  <span class="co"># Negative because we&#39;re doing gradient ascent</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    policy_loss <span class="op">=</span> torch.stack(policy_loss).<span class="bu">sum</span>()</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    policy_loss.backward()</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code></pre></div>
<h2 id="implementation-details">Implementation Details</h2>
<ul>
<li>Steps 1-3 are implemented in the main training loop and the
<code>select_action</code> function.</li>
<li>Step 4 (normalization) is done in the <code>update_policy</code>
function.</li>
<li>Steps 5-6 are implemented in the <code>update_policy</code>
function, where we compute the policy loss and perform
backpropagation.</li>
</ul>
<p>The key part is in the <code>update_policy</code> function:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>policy_loss <span class="op">=</span> []</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> log_prob, R <span class="kw">in</span> <span class="bu">zip</span>(log_probs, returns):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    policy_loss.append(<span class="op">-</span>log_prob <span class="op">*</span> R)</span></code></pre></div>
<p>This line implements the policy gradient. The negative sign is
present because weâ€™re doing gradient descent to minimize the negative
expected return (which is equivalent to maximizing the expected
return).</p>
<p>The <code>backward()</code> call computes the gradients, and the
<code>optimizer.step()</code> updates the parameters, implementing step
6 of our algorithm.</p>
<p>By iterating this process over many episodes, the policy gradually
improves, learning to maximize the expected return in the given
environment.</p>
<h2 id="mathematical-explanation-of-the-update-step">Mathematical
Explanation of the Update Step</h2>
<ol type="1">
<li><p>We calculate the returns <span class="math inline">\(G_t\)</span>
for each timestep:</p>
<p><span class="math inline">\(G_t = \sum_{k=0}^{T-t} \gamma^k
r_{t+k}\)</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is the discount
factor (0.99 in this code).</p></li>
<li><p>We normalize the returns to reduce variance:</p>
<p><span class="math inline">\(G_t&#39; = \frac{G_t -
\mu(G)}{\sigma(G)}\)</span></p></li>
<li><p>The policy loss for each step is calculated as:</p>
<p><span class="math inline">\(L_t = -\log \pi_\theta(a_t|s_t)
G_t&#39;\)</span></p></li>
<li><p>The total loss is the sum of these individual losses:</p>
<p><span class="math inline">\(L = \sum_t L_t\)</span></p></li>
<li><p>We then use backpropagation to compute <span
class="math inline">\(\nabla_\theta L\)</span> and update our policy
parameters:</p>
<p><span class="math inline">\(\theta \leftarrow \theta - \alpha
\nabla_\theta L\)</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the learning
rate.</p></li>
</ol>
<p>following the core principles of the REINFORCE algorithm</p>
<p><a href="../index.html">home</a></p>
</body>
</html>
